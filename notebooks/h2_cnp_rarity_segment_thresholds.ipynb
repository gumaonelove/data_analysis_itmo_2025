{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2: CNP + Редкость устройств и сегментные пороги\n",
    "\n",
    "**H2 (Product):** Редкие/новые устройства и отсутствующая карта (CNP) ↑ риск → device binding и лимиты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "ROOT = Path().resolve()\n",
    "if not (ROOT/'src').exists(): ROOT = ROOT.parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print('Project root:', ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval': {'roc_auc': 0.5001122214983779,\n",
       "  'pr_auc': 0.1980998640895631,\n",
       "  'threshold_at_precision': 0.9,\n",
       "  'thr_value': None,\n",
       "  'recall_at_precision': None,\n",
       "  'precision_achieved': None},\n",
       " 'segment_thr': {'thr_seg': None,\n",
       "  'rec_seg': None,\n",
       "  'p_seg': None,\n",
       "  'thr_glb': None,\n",
       "  'rec_glb': None,\n",
       "  'p_glb': None},\n",
       " 'policy_metrics': {'Precision': 0.0, 'Recall': 0.0, 'TrafficShare': 0.0}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from src.data import load_transactions, load_fx\n",
    "from src.currency import convert_to_usd\n",
    "from src.features import unpack_last_hour_activity, add_basic_time_features, customer_velocity\n",
    "from src.validation import split_time_aware\n",
    "from src.pipeline import build_preprocessor, build_logreg\n",
    "from src.eval import eval_pack, thr_at_precision\n",
    "from src.rarity import train_freqs, apply_freqs\n",
    "from src.segments import mask_super_risky\n",
    "\n",
    "DATA=Path('../data'); TX=DATA/'transaction_fraud_data.parquet'; FX=DATA/'historical_currency_exchange.parquet'\n",
    "df = load_transactions(TX); fx = load_fx(FX)\n",
    "df = convert_to_usd(df, fx); df = unpack_last_hour_activity(df); df = add_basic_time_features(df); df = customer_velocity(df)\n",
    "\n",
    "train, test = split_time_aware(df)\n",
    "y_tr, y_te = train['is_fraud'].astype(int), test['is_fraud'].astype(int)\n",
    "X_tr, X_te = train.drop(columns=['is_fraud']), test.drop(columns=['is_fraud'])\n",
    "\n",
    "dev_freq, ip_freq = train_freqs(train)\n",
    "X_tr_r = apply_freqs(X_tr, dev_freq, ip_freq); X_te_r = apply_freqs(X_te, dev_freq, ip_freq)\n",
    "\n",
    "pipe = Pipeline([('prep', build_preprocessor(X_tr_r)), ('clf', build_logreg())]).fit(X_tr_r, y_tr)\n",
    "p = pipe.predict_proba(X_te_r)[:,1]\n",
    "\n",
    "seg = mask_super_risky(test)\n",
    "thr_seg, rec_seg, p_seg = thr_at_precision(y_te[seg], p[seg], 0.90) if seg.any() else (None,None,None)\n",
    "thr_glb, rec_glb, p_glb = thr_at_precision(y_te[~seg], p[~seg], 0.90)\n",
    "decision = np.where(seg, p >= (thr_seg or 1.0), p >= (thr_glb or 1.0)).astype(int)\n",
    "\n",
    "{'eval': eval_pack(y_te, p),\n",
    " 'segment_thr': {'thr_seg':thr_seg, 'rec_seg':rec_seg, 'p_seg':p_seg, 'thr_glb':thr_glb, 'rec_glb':rec_glb, 'p_glb':p_glb},\n",
    " 'policy_metrics': {'Precision': precision_score(y_te, decision, zero_division=0),\n",
    "                    'Recall': recall_score(y_te, decision, zero_division=0),\n",
    "                    'TrafficShare': float(decision.mean())}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470ecfb",
   "metadata": {},
   "source": [
    "### Выводы по H2 (CNP + редкость устройств)\n",
    "\n",
    "**Факты**\n",
    "- Качество модели на текущих фичах ≈ случайное: **ROC-AUC ~0.500**, **PR-AUC ~0.198** (на уровне доли fraud).\n",
    "- Достичь **Precision ≥ 0.90** не удалось (порог не найден для сегмента и вне сегмента).\n",
    "- Двухпороговая политика не сработала: `Precision=0`, `Recall=0`, `TrafficShare=0` (фоллбек порога привёл к нулевым алертам).\n",
    "\n",
    "**Интерпретация причин**\n",
    "1. **Сигнал редкости «схлопнулся» на тесте**: большинство `device_fingerprint`/`ip_address` оказались «новыми» (freq=0), либо почти все — редкие → модель не различает риск внутри сегмента.  \n",
    "2. **Флаги сегмента (CNP, cross-border, high-risk)** могли быть **слабо вариативны** или перекошены по времени/странам → сегмент S слишком мал/неотличим.  \n",
    "3. **Пре-процессинг**: агрессивный `min_frequency` в OHE и медианная импутация могли обесценить слабые сигналы.\n",
    "\n",
    "**Решения (минимальные доработки, чтобы «зажёгся» сигнал)**\n",
    "- Усилить признаки редкости:\n",
    "  - `device_is_new = (device_freq == 0)`\n",
    "  - `ip_is_new = (ip_freq == 0)`\n",
    "  - `device_is_rare = (device_freq <= 3)`\n",
    "  - `ip_is_rare = (ip_freq <= 3)`\n",
    "  - `rarity_score = 1.0 / (1 + device_freq) + 1.0 / (1 + ip_freq)`  \n",
    "  и **взаимодействия** с `is_card_present==False`, `is_outside_home_country==True`.\n",
    "- Смягчить кодирование категорий: снизить `min_frequency` в `OneHotEncoder` до **10**.\n",
    "- Проверить наполнение сегмента:\n",
    "  - `test['is_card_present'].value_counts(normalize=True)`,\n",
    "  - `test['is_outside_home_country'].mean()`,\n",
    "  - `test['is_high_risk_vendor'].mean()`,\n",
    "  - `seg.sum()` — если `seg` слишком мал, используйте более широкий сегмент (например, CNP ∪ cross-border).\n",
    "- Как «страховочную» политику до обучения лучшей модели — применять **квантильные пороги по amount_usd** внутри сегментов (P95 для CNP, P90 для cross-border, P80 для CNP∩CB∩HRV).\n",
    "\n",
    "Итог по H2 на текущем запуске: гипотеза не подтверждена. Для следующей итерации добавляем «жёсткие» признаки новизны/редкости + взаимодействия, расширяем сегмент (если он мал), смягчаем OHE. Параллельно держим квантильную политику как fallback до появления устойчивого модельного сигнала."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad7ce6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
