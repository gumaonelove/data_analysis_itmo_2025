{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1: Velocity + Временные признаки\n",
    "\n",
    "**H1 (Product):** Скорость операций и cross-border транзакции повышают риск → шаг-up аутентификация при превышении порогов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "ROOT = Path().resolve()\n",
    "if not (ROOT/'src').exists(): ROOT = ROOT.parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print('Project root:', ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'roc_auc': 0.5001122214983779,\n",
       "  'pr_auc': 0.1980998640895631,\n",
       "  'threshold_at_precision': 0.9,\n",
       "  'thr_value': None,\n",
       "  'recall_at_precision': None,\n",
       "  'precision_achieved': None},\n",
       " {'roc_auc': 0.5001122214983779,\n",
       "  'pr_auc': 0.1980998640895631,\n",
       "  'threshold_at_precision': 0.9,\n",
       "  'thr_value': None,\n",
       "  'recall_at_precision': None,\n",
       "  'precision_achieved': None},\n",
       " {'delta_pr': 0.0, '95%CI': (0.0, 0.0)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.data import load_transactions, load_fx\n",
    "from src.currency import convert_to_usd\n",
    "from src.features import unpack_last_hour_activity, add_basic_time_features, customer_velocity\n",
    "from src.validation import split_time_aware, bootstrap_pr_auc\n",
    "from src.pipeline import build_preprocessor, build_logreg\n",
    "from src.eval import eval_pack\n",
    "\n",
    "DATA=Path('../data'); TX=DATA/'transaction_fraud_data.parquet'; FX=DATA/'historical_currency_exchange.parquet'\n",
    "df = load_transactions(TX); fx = load_fx(FX)\n",
    "df = convert_to_usd(df, fx); df = unpack_last_hour_activity(df); df = add_basic_time_features(df)\n",
    "\n",
    "# Baseline\n",
    "train, test = split_time_aware(df)\n",
    "y_tr, y_te = train['is_fraud'].astype(int), test['is_fraud'].astype(int)\n",
    "X_tr, X_te = train.drop(columns=['is_fraud']), test.drop(columns=['is_fraud'])\n",
    "pipe_base = Pipeline([('prep', build_preprocessor(X_tr)), ('clf', build_logreg())]).fit(X_tr, y_tr)\n",
    "p_base = pipe_base.predict_proba(X_te)[:,1]\n",
    "\n",
    "# Candidate: +velocity\n",
    "df_v = customer_velocity(df)\n",
    "train_v, test_v = split_time_aware(df_v)\n",
    "y_tr_v, y_te_v = train_v['is_fraud'].astype(int), test_v['is_fraud'].astype(int)\n",
    "X_tr_v, X_te_v = train_v.drop(columns=['is_fraud']), test_v.drop(columns=['is_fraud'])\n",
    "pipe_v = Pipeline([('prep', build_preprocessor(X_tr_v)), ('clf', build_logreg())]).fit(X_tr_v, y_tr_v)\n",
    "p_v = pipe_v.predict_proba(X_te_v)[:,1]\n",
    "\n",
    "base_metrics = eval_pack(y_te, p_base); cand_metrics = eval_pack(y_te_v, p_v)\n",
    "delta, lo, hi = bootstrap_pr_auc(y_te_v.to_numpy(), p_base, p_v, B=500)\n",
    "base_metrics, cand_metrics, {'delta_pr':delta, '95%CI':(lo,hi)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc70934",
   "metadata": {},
   "source": [
    "### Краткий вывод по H1 (Velocity + время):\n",
    "* Baseline и кандидат с velocity/time показывают одинаковые метрики: ROC-AUC ≈ 0.5001, PR-AUC ≈ 0.1981 (≈ доля позитивов), ∆PR-AUC = 0.0 с 95% CI = (0.0; 0.0).\n",
    "* Целевой Precision ≥ 0.90 недостижим на текущем скоупе признаков (порог не найден).\n",
    "\n",
    "### Интерпретация\n",
    "\n",
    "Модель находится на уровне случайного угадывания; добавление velocity/времени не дало сигнала. Вероятные причины:\n",
    "1.\tПотеря информативности из-за препроцессинга: жесткий порог min_frequency=50 в OHE мог выкинуть большинство категорий; числовые признаки могли стать почти константными после импутации.\n",
    "2.\tПустые/некорректные velocity-окна: если временной индекс/гранулярность не позволяют посчитать rolling, новые фичи заполняются нулями/NaN и не помогают.\n",
    "3.\tНеверная конвертация сумм (массовые NaN после join с FX по дате → затем медианная импутация превращает amount_usd в слабый сигнал).\n",
    "\n",
    "### Что сделать дальше (минимальные правки)\n",
    "* Снизить min_frequency в OneHotEncoder до 10 или убрать, чтобы не терять категориальные признаки.\n",
    "* Провести sanity-чек фичей после препроцессинга/импутации: доля NaN, доля константных/квази-константных колонок, дисперсии.\n",
    "* Проверить конвертацию валют: долю NaN в rate_to_currency после join и покрытие дат (при необходимости — ffill по дате).\n",
    "* Убедиться, что customer_velocity реально создаёт вариацию (распределения cust_tx_count_{1,6,24}h, time_since_prev_s ≠ константа).\n",
    "* Как бенчмарк: запустить дерево/LightGBM (умеют работать с пропусками) — если качество растёт, проблема именно в признаках/импутации для логистической регрессии.\n",
    "\n",
    "Итог по H1 на текущем срезе: гипотеза не подтверждена; необходима донастройка препроцессинга и валидация корректности источников сигнала (velocity, суммы, категории)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89788f26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
