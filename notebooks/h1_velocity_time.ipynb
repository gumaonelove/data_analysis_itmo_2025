{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1: Velocity + Временные признаки\n",
    "\n",
    "**H1 (Product):** Скорость операций и cross-border транзакции повышают риск → шаг-up аутентификация при превышении порогов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "ROOT = Path().resolve()\n",
    "if not (ROOT/'src').exists(): ROOT = ROOT.parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print('Project root:', ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'roc_auc': 0.971676410870483,\n",
       "  'pr_auc': 0.9249518117159337,\n",
       "  'threshold_at_precision': 0.9,\n",
       "  'thr_value': 0.3122758056285582,\n",
       "  'recall_at_precision': 0.7514503899187782,\n",
       "  'precision_achieved': 0.9000028278600775},\n",
       " {'roc_auc': 0.971681985928168,\n",
       "  'pr_auc': 0.9249480817960043,\n",
       "  'threshold_at_precision': 0.9,\n",
       "  'thr_value': 0.22828390395114512,\n",
       "  'recall_at_precision': 0.7518113008985672,\n",
       "  'precision_achieved': 0.9000024227153794},\n",
       " {'delta_pr': -2.6066334293508573e-06,\n",
       "  '95%CI': (-3.159779047487432e-05, 2.5990901927425147e-05)})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from src.data import load_transactions, load_fx\n",
    "from src.currency import convert_to_usd\n",
    "from src.features import unpack_last_hour_activity, add_basic_time_features, customer_velocity, clip_and_fill\n",
    "from src.validation import compute_time_cutoff, split_by_cutoff\n",
    "from src.pipeline import build_preprocessor, build_logreg\n",
    "from src.eval import eval_pack\n",
    "from src.validation import bootstrap_pr_auc\n",
    "\n",
    "# 1) Загрузка и подготовка\n",
    "DATA = Path('../data')\n",
    "TX = DATA/'transaction_fraud_data.parquet'\n",
    "FX = DATA/'historical_currency_exchange.parquet'\n",
    "\n",
    "df = load_transactions(TX)\n",
    "fx = load_fx(FX)\n",
    "\n",
    "df = convert_to_usd(df, fx)\n",
    "df = unpack_last_hour_activity(df)\n",
    "df = add_basic_time_features(df)\n",
    "\n",
    "# ВАЖНО: сначала клиппинг/заполнение (убираем хвосты/пропуски ДО обучения)\n",
    "df = clip_and_fill(df)\n",
    "\n",
    "# 2) Кандидат: velocity (и тоже клиппинг/заполнение на тех же строках)\n",
    "df_v = customer_velocity(df)\n",
    "df_v = clip_and_fill(df_v)\n",
    "\n",
    "# 3) Единый time split (один cutoff — два сплита)\n",
    "cutoff = compute_time_cutoff(df, ts_col='timestamp', test_size=0.2)\n",
    "\n",
    "train, test   = split_by_cutoff(df, cutoff, ts_col='timestamp')\n",
    "train_v, test_v = split_by_cutoff(df_v, cutoff, ts_col='timestamp')\n",
    "\n",
    "# Контроль, что разбиения совпали по индексам\n",
    "assert train.index.equals(train_v.index)\n",
    "assert test.index.equals(test_v.index)\n",
    "\n",
    "# 4) Обучение: baseline\n",
    "y_tr, y_te = train['is_fraud'].astype(int), test['is_fraud'].astype(int)\n",
    "X_tr, X_te = train.drop(columns=['is_fraud']), test.drop(columns=['is_fraud'])\n",
    "\n",
    "pipe_base = Pipeline([\n",
    "    ('prep', build_preprocessor(X_tr)),\n",
    "    ('clf', build_logreg()),\n",
    "]).fit(X_tr, y_tr)\n",
    "\n",
    "p_base = pipe_base.predict_proba(X_te)[:, 1]\n",
    "\n",
    "# 5) Обучение: + velocity\n",
    "y_tr_v, y_te_v = train_v['is_fraud'].astype(int), test_v['is_fraud'].astype(int)\n",
    "X_tr_v, X_te_v = train_v.drop(columns=['is_fraud']), test_v.drop(columns=['is_fraud'])\n",
    "\n",
    "pipe_v = Pipeline([\n",
    "    ('prep', build_preprocessor(X_tr_v)),\n",
    "    ('clf', build_logreg()),\n",
    "]).fit(X_tr_v, y_tr_v)\n",
    "\n",
    "p_v = pipe_v.predict_proba(X_te_v)[:, 1]\n",
    "\n",
    "# 6) Метрики (одинаковая тестовая выборка!)\n",
    "base_metrics = eval_pack(y_te, p_base)\n",
    "cand_metrics = eval_pack(y_te_v, p_v)\n",
    "\n",
    "# 7) Bootstrap дельты AP с согласованным y_true\n",
    "delta, lo, hi = bootstrap_pr_auc(y_te.to_numpy(), p_base, p_v, B=1000, seed=42)\n",
    "\n",
    "base_metrics, cand_metrics, {'delta_pr': delta, '95%CI': (lo, hi)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd30c7f",
   "metadata": {},
   "source": [
    "Отлично. Результаты выглядят стабильными и «здоровыми» с точки зрения численной устойчивости:\n",
    "* ROC AUC: 0.971676 → 0.971682 (≈ +0.000006)\n",
    "* PR AUC: 0.924952 → 0.924948 (≈ −0.000004)\n",
    "* Recall @ P≥0.90: 0.75145 → 0.75181 (незначительный рост)\n",
    "* Порог для P≈0.90: 0.312 → 0.228 (вариант достигает той же точности при меньшем пороге)\n",
    "* ΔPR (bootstrap, B=1000): −2.6e−06, 95% CI [−3.16e−05; +2.60e−05] → статзначимого прироста AP нет\n",
    "\n",
    "Вывод по H1 сейчас\n",
    "\n",
    "В текущей реализации velocity-признаки не дают статистически значимого прироста AP на общей выборке. При этом наблюдается слабый сдвиг в сторону более «щедрой» отсечки (ниже порог при той же Precision) и микроприращение recall @ P≥0.90. Это может быть полезно локально по сегментам (например, cross-border), но не видно эффекта «в среднем по больнице».\n",
    "\n",
    "Ниже — короткий план, как оперативно «дожать» гипотезу и принять продуктовое решение.\n",
    "\n",
    "⸻\n",
    "\n",
    "Что сделать дальше (приоритетно)\n",
    "\t1.\tПроверить uplift по целевым сегментам (там, где H1 задумана):\n",
    "\n",
    "* is_outside_home_country == 1 (cross-border),\n",
    "* высокорисковые категории мерчантов,\n",
    "* ночные часы/выходные.\n",
    "\n",
    "Если uplift концентрируется в этих зонах — включаем step-up только в них, а не глобально.\n",
    "\t2.\tПосмотреть вклад именно velocity-фичей в модели:\n",
    "\n",
    "* знак/величина коэффициентов (odds ratio),\n",
    "* монотония эффектов (рост риска при росте скорости).\n",
    "\n",
    "\t3.\tДобавить интеракции «velocity × cross-border / high-risk / is_card_present»:\n",
    "\n",
    "* именно такие взаимодействия чаще всего «раскрывают» H1.\n",
    "\n",
    "\t4.\tПроверить устойчивость во времени (rolling backtest, 3–5 временных фолдов) — чтобы убедиться, что эффект не артефакт единственного среза.\n",
    "\t5.\tПересчитать бизнес-метрики на уровне порога P≥0.90:\n",
    "\n",
    "* прирост True Positive, рост нагрузки на step-up (FP) и итоговый net benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc80170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xc/r2jvff2n1_s_9sgktp8v3q300000gn/T/ipykernel_8332/2815852813.py:19: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask_vel = coef_v['feature'].str.contains(r'(time_since_prev_s|cust_tx_count_1h|cust_tx_count_6h|cust_tx_count_24h)')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>num__cust_tx_count_24h</td>\n",
       "      <td>0.051656</td>\n",
       "      <td>1.053013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>num__time_since_prev_s</td>\n",
       "      <td>0.037748</td>\n",
       "      <td>1.038469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>num__cust_tx_count_6h</td>\n",
       "      <td>-0.010339</td>\n",
       "      <td>0.989714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>num__cust_tx_count_1h</td>\n",
       "      <td>-0.045046</td>\n",
       "      <td>0.955954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature      coef  odds_ratio\n",
       "15  num__cust_tx_count_24h  0.051656    1.053013\n",
       "12  num__time_since_prev_s  0.037748    1.038469\n",
       "14   num__cust_tx_count_6h -0.010339    0.989714\n",
       "13   num__cust_tx_count_1h -0.045046    0.955954"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Коэффициенты/odds ratio, выделить velocity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def coef_table(pipe):\n",
    "    prep = pipe.named_steps['prep']\n",
    "    clf  = pipe.named_steps['clf']\n",
    "    feat = prep.get_feature_names_out()\n",
    "    coefs = clf.coef_.ravel()\n",
    "    dfc = pd.DataFrame({'feature': feat, 'coef': coefs})\n",
    "    dfc['odds_ratio'] = np.exp(dfc['coef'])\n",
    "    return dfc.sort_values(dfc.columns[dfc.columns.get_loc('coef')], key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "coef_base = coef_table(pipe_base)\n",
    "coef_v    = coef_table(pipe_v)\n",
    "\n",
    "# velocity-фичи\n",
    "mask_vel = coef_v['feature'].str.contains(r'(time_since_prev_s|cust_tx_count_1h|cust_tx_count_6h|cust_tx_count_24h)')\n",
    "coef_v[mask_vel].sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5e2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg</th>\n",
       "      <th>n</th>\n",
       "      <th>ap_base</th>\n",
       "      <th>ap_var</th>\n",
       "      <th>rec@P0.90_base</th>\n",
       "      <th>rec@P0.90_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cross_border</td>\n",
       "      <td>479994</td>\n",
       "      <td>0.942141</td>\n",
       "      <td>0.942171</td>\n",
       "      <td>0.773749</td>\n",
       "      <td>0.774714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>night</td>\n",
       "      <td>326051</td>\n",
       "      <td>0.967413</td>\n",
       "      <td>0.967243</td>\n",
       "      <td>0.913099</td>\n",
       "      <td>0.912516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high_risk_vendor</td>\n",
       "      <td>374684</td>\n",
       "      <td>0.919979</td>\n",
       "      <td>0.920010</td>\n",
       "      <td>0.735781</td>\n",
       "      <td>0.736092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seg       n   ap_base    ap_var  rec@P0.90_base  rec@P0.90_var\n",
       "0      cross_border  479994  0.942141  0.942171        0.773749       0.774714\n",
       "1             night  326051  0.967413  0.967243        0.913099       0.912516\n",
       "2  high_risk_vendor  374684  0.919979  0.920010        0.735781       0.736092"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Uplift по сегментам (PR AUC и Recall@P≥0.90)\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "def recall_at_precision(y, p, target=0.90):\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    m = prec[:-1] >= target\n",
    "    return float(rec[:-1][m].max()) if m.any() else None\n",
    "\n",
    "def segment_report(mask, name):\n",
    "    y = y_te_v[mask]\n",
    "    b = p_base[mask]\n",
    "    v = p_v[mask]\n",
    "    rep = {\n",
    "        'seg': name,\n",
    "        'n': int(mask.sum()),\n",
    "        'ap_base': float(average_precision_score(y, b)),\n",
    "        'ap_var':  float(average_precision_score(y, v)),\n",
    "        'rec@P0.90_base': recall_at_precision(y, b, 0.90),\n",
    "        'rec@P0.90_var':  recall_at_precision(y, v, 0.90),\n",
    "    }\n",
    "    return rep\n",
    "\n",
    "reports = []\n",
    "# 2.1 Cross-border\n",
    "reports.append(segment_report(test_v['is_outside_home_country'] == 1, 'cross_border'))\n",
    "# 2.2 Ночь (например, 0-6)\n",
    "reports.append(segment_report((test_v['tx_hour']>=0) & (test_v['tx_hour']<=6), 'night'))\n",
    "# 2.3 High-risk vendor\n",
    "if 'is_high_risk_vendor' in test_v.columns:\n",
    "    reports.append(segment_report(test_v['is_high_risk_vendor']==1, 'high_risk_vendor'))\n",
    "\n",
    "pd.DataFrame(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0659fb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Users/gumerovbr/Documents/GitHub/data_analysis_itmo_2025/.venv/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.9717569885306397,\n",
       " 'pr_auc': 0.9253000437697738,\n",
       " 'threshold_at_precision': 0.9,\n",
       " 'thr_value': 0.5556421146023413,\n",
       " 'recall_at_precision': 0.7528839148385008,\n",
       " 'precision_achieved': 0.9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Интеракции (быстрый прототип)\n",
    "\n",
    "# Добавьте к df_v взаимодействия перед обучением:\n",
    "\n",
    "df_vi = df_v.copy()\n",
    "df_vi['vel1_x_xborder'] = df_vi['cust_tx_count_1h'] * df_vi['is_outside_home_country']\n",
    "df_vi['vel6_x_xborder'] = df_vi['cust_tx_count_6h'] * df_vi['is_outside_home_country']\n",
    "df_vi['vel24_x_xborder']= df_vi['cust_tx_count_24h'] * df_vi['is_outside_home_country']\n",
    "\n",
    "# Дальше тот же fixed cutoff split и обучение, как у df_v\n",
    "train_vi, test_vi = split_by_cutoff(df_vi, cutoff, ts_col='timestamp')\n",
    "y_tr_vi, y_te_vi = train_vi['is_fraud'].astype(int), test_vi['is_fraud'].astype(int)\n",
    "X_tr_vi, X_te_vi = train_vi.drop(columns=['is_fraud']), test_vi.drop(columns=['is_fraud'])\n",
    "\n",
    "pipe_vi = Pipeline([('prep', build_preprocessor(X_tr_vi)), ('clf', build_logreg())]).fit(X_tr_vi, y_tr_vi)\n",
    "p_vi = pipe_vi.predict_proba(X_te_vi)[:,1]\n",
    "\n",
    "from src.eval import eval_pack\n",
    "eval_pack(y_te_vi, p_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb502762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Rolling backtest (устойчивость во времени)\n",
    "\n",
    "def rolling_splits(df, ts_col='timestamp', k=3):\n",
    "    df = df.sort_values(ts_col).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    fold = []\n",
    "    for i in range(1, k+1):\n",
    "        cut = int(n*(1 - i/(k+1)))\n",
    "        train = df.iloc[:cut]\n",
    "        test  = df.iloc[cut:]\n",
    "        fold.append((train, test))\n",
    "    return fold\n",
    "\n",
    "def evaluate_pipe(train, test):\n",
    "    y_tr, y_te = train['is_fraud'].astype(int), test['is_fraud'].astype(int)\n",
    "    X_tr, X_te = train.drop(columns=['is_fraud']), test.drop(columns=['is_fraud'])\n",
    "    pipe = Pipeline([('prep', build_preprocessor(X_tr)), ('clf', build_logreg())]).fit(X_tr, y_tr)\n",
    "    p = pipe.predict_proba(X_te)[:,1]\n",
    "    return eval_pack(y_te, p)\n",
    "\n",
    "for name, data in [('baseline', df), ('velocity', df_v)]:\n",
    "    results = []\n",
    "    for tr, te in rolling_splits(data, 'timestamp', k=3):\n",
    "        results.append(evaluate_pipe(tr, te))\n",
    "    print(name, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Калибровка и Brier score (на случай продуктового порога)\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "brier_base = brier_score_loss(y_te, p_base)\n",
    "brier_var  = brier_score_loss(y_te_v, p_v)\n",
    "brier_base, brier_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc70934",
   "metadata": {},
   "source": [
    "Решение «по продукту» (что зафиксировать)\n",
    "* Если uplift появляется в целевых сегментах (например, cross-border) — оставляем velocity-признаки и включаем step-up только в этих сегментах. На общей популяции — эффект ≈0.\n",
    "* Если uplift нет даже по сегментам — фиксируем, что H1 не подтверждена в текущем виде; пробуем интеракции (см. 3) и/или бининг velocity в монотоничные сплайны/квантили.\n",
    "* В любом случае: у вас теперь реплицируемый эксперимент (единый cutoff, стабильные пайплайны). Сохраните артефакты (версия данных, cutoff, seed, метрики) в отчёт, чтобы закрыть гипотезу формально."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
